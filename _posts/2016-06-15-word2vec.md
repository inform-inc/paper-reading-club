---
layout: post
title:  Word2Vec
date:   2016-06-15 12:00:00 -0700
categories: papers
---

## Introduction

Hi Palo Alto Engineering Team,

This week's topic is Word2Vec. Theban is lecturing/moderating this week's "Papers We Love" discussion. He has two papers for us to glance/skim/read.

1. [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
2. [Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781v3.pdf)

Here's quora's answer to what word2vec is:

The TL;DR version:

> word2vec is not one algorithm.
> word2vec is not deep learning.
> The recommended algorithm in word2vec, skip-gram with negative sampling, factorizes a word-context PMI matrix.

The Long Answer:
> It's complicated.

We can have the discussion over lunch and/or getting lunch.

## Notes

`TBA`
